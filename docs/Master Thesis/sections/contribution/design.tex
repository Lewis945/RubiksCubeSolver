\documentclass[../../main.tex]{subfiles}

\begin{document}

% Start Semester 1

The goal of the project was to rewrite to the newer version of the OpenCV and improve software of Artjom and others. 

\subsubsection{Extraction}

This part was partially modified. Basically contour detection was left as it is but logic of the usage was changed. If contours have not been found for thresholding bounds program starts increasing upper bound and decreasing lower bound until any sufficient contour is found. If they exist than maximal contour is taken, it is supposed to be the largest. Approximation of contour is the next step, while in the previous work they extracted bounding rectangle from the contour, I found out that standard OpenCV methods usually return not very appropriate bounding rectangles even though any contour is detected correctly. I checked approximation method and found out that it also returned points, which were lines intersections, and in that case, they were corners of the grid. These points were perfectly identified and pointed straight to where they should be.

\subsubsection{Transformation}

In this section code was pretty much changed while the meaning is left as it is. Perspective transformation is applied and perspective matrix is constructed using corner points from the extraction part. I computed width and height of the new image and using this shape and corner points OpenCV constructs perspective matrix.

\subsubsection{Flooding}

It was completely changed. In the previous version, they were looking for all the small connected components and just coloring them into white (background color of thresholded image). In my case, I used the OpenCV method fastNlMeansDenoising which removes noise from the image. I practically found parameters, which gave the best results and good performance. Image after this method was partially blurred so I changed all the pixels that are less than 128 to 0 and others to 255. On this stage image was also inverted which means that we have black background and white numbers and grid lines.

\subsubsection{Segmentation}

This section was also modified pretty much. I cut the image into 81 equal pieces, the same was done in the previous work. Then I went through all the pieces and check whether they contained a number or not. I did that as the previous researches but I selected area, which was half of the size of the piece and was centered. If the sum of the pixels inside was equal to 0 it means that it was empty. Otherwise, piece of the image was sent to the clear_borders method, which picks the middle point and finds all the contours. Algorithm works as follows, it tries to find the biggest and the closest to the center point contour. Found contour is supposed to be our number. Then it expands the size of the image until it is not equal to 40. Segmented images are stored in the matrix of image arrays and Nones.

\subsubsection{OCR}

For the OCR it was decided to use KNearest neighbor classification algorithm instead of SVM. It was much easier to set it up, while the results are the same. The logic beneath is quite the same, it is trained with a set of vectorized data as in the previous work but the logic for classification is different. The output of the algorithm is class membership. Neighbors are voting and the one who earns the majority wins, which means that input will gain that class which is the most common among its neighbors.

% End Semester 1

% Start Semester 2

\subsubsection{Thresholding}

Nowadays, computer vision application can’t process colored images so the need to convert input image to grayscale (monochrome). There are two basic methods for thresholding: global thresholding and adaptive thresholding. Global thresholding is the simplest method. It takes an image, goes through all the pixels, calculates intensity ((R+G+B)/3) and compare it to a global thresholding value (256/2 = 128). If a pixel intensity is larger than thresholding value than the pixel is converted to white, otherwise to black. Global thresholding it is not useful in real world.

\begin{figure} [ht]
\begin{center}
\includegraphics[width=\textwidth]{images/related/semester1/comparisonOfImageThresholding}
\caption{Comparison of image thresholding}
\label{fig:comparisonOfImageThresholding}
\end{center}
\end{figure}

Adaptive thresholding is used to get a better result. It does not use a fixed thresholding value (128), the value is calculated for each pixel separately. For instance, it takes 11*11 (121) surrounding pixels, computes the sum of their intensities and divides it by the number of pixels (121). If the mean intensity of the current pixel, located in the center of that area, is greater than the mean thresholding value, it will become white, otherwise black. This algorithm needs to do all of that for each pixel so it is much slower than the previous but it gives much better results.

\begin{figure} [ht]
\begin{center}
\includegraphics[width=\textwidth]{images/related/semester1/findingLogicOfAdaptiveThresholdingValue}
\caption{Finding logic of adaptive thresholding value}
\label{fig:findingLogicOfAdaptiveThresholdingValue}
\end{center}
\end{figure}

\subsubsection{Contour and corner detection}

In order to start the image processing adaptive thresholding was applied so the returned image was suitable for finding and extracting contours. Basically, lines on that image were turned to white while the background and other light pixels were turned to black. I assumed that image or video contain standard Rubik’s cube, which is 9 by 9.
Code responsible for finding contours iterates through all the contours and stores threshold-passed contours to an array. A contour is a list of points. Its representation can be different depending on the circumstances. In order to find a contour in a binary image all connected components (pixels connected to each other) in the image should be considered.
From scratch, I ran into a problem that edges of cube are not very clear so I was not able to extract faces just by finding bounding boxes for faces contours. In contrary, I decided to use contours that were detected with better quality, contours of each piece of Rubik’s cube. So as a first step I was trying to find all the contours that are closed and whose approximation gives 4 corners, which means that they might be our diffused square pieces of cube’s face. Then I decided to compute remove duplicates, they occur sometimes if OpenCV sees a couple of different contours around the same object, so I took all contours with almost the same center of mass and remove all of them but one. At this point I computed corners of these distinct contours to the horizon and if there 9 of them with the same angle then we found our face. The next step is quite simple, find the extremums or top-left, top-right, bottom-left, bottom-right points, depending on the angle of our face, these points will be the corners of the face. 
\subsubsection{Extraction}

Since detection of corners for each face is described in the previous section, at this point, we have 4 corners and simply need to be passed to the next step where linear transformation will be applied. As a result, plane image of a face will be stored.
Limitations. It is not possible to detect face corners and extract the actual face from the image if an image or video quality is quite poor, contours are broken or etc. However, if the source quality is good enough than this algorithm is quite sufficient because of its simplicity and performance.

\subsubsection{Hough Transformation}

\begin{figure} [ht]
\begin{center}
\includegraphics[width=\textwidth]{images/related/semester1/houghLinesTransformation}
\caption{Hough lines transformation}
\label{fig:houghLinesTransformation}
\end{center}
\end{figure} 

Hough transformation is a numerical method algorithm used to extract features from an image. In this case, it extracts lines. It skips all the white pixels and draws 180 virtual lines through each black pixel. Pixels ‘vote’ for the lines and the virtual line with the largest number of votes in the accumulator is the winner and most probably the real line.
This algorithm is another alternative option for extracting face from the source (image or video).

\subsubsection{Transforming Image}

After face corners coordinates were received, program extracts them and apply linear transformation in order to get a square shape image. For this purpose was decided to use perspective transformation equations:
x=(a×x+b×y+c)/(g ×x+h×y+1)
y=(d×x+e×y+c)/(g ×x+h×y+1)

There are 8 unknown variables in these two equations but they could be found with construction of 8 equations system (four for each of two equations). With this system of equations it is possible to map points from distorted image to a flat square image. Basically, in the program, OpenCV’s GetPerspectiveTransform and WarpPerspective methods were used in order to apply perspective transformation to a source.

\subsubsection{Identifying unique faces}

Firstly, it was thought to use SURF to determine image features and compare faces on this basis but since in this project only the solved cube was used it turned to use a Histogram comparison as a better choice for this case.
It was decided to keep all uniquely detected faces in the list and compare each newly detected one with all from the list. In order to do that it is needed to convert images to HSV, then calculate Histogram from HSV images and normalize them. In the end, just compare images with one of possible methods, in this case correlation was used, it worked fine. Basically, if the correlation is more than 0.9 it is assumed that faces are to similar and it could be the same face.
This algorithm might not be the better choice but in the current project it worked as it was expected.

\subsubsection{Color detection}

Color detection is nearly the easiest part of the program. In order to detect RGB color representation for each piece of cube face it is simply needed to segment image into n equal squares, since in this project 3 by 3 Rubik’s cube was used, segmentation produced 9 pieces and this code won’t work for larger cubes.
After the piece of face is cut out just take the pixel in the center and get its intensity, that is all, RGB color detected. Collect them to some data structure and, basically, the process is finished.

% End Semester 2

% Start Semester 3

\subsubsection{Semester 3}
Since this seminar’s project is a part of my master thesis I have not created my own approach. On the contrary, I have implemented my own version of the described algorithm in C sharp and .NET platform.

% End Semester 3

\end{document}

