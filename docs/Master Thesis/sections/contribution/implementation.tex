\documentclass[../../main.tex]{subfiles}

\begin{document}

Implementation was done using .NET {\Csharp} and its wrappers for OpenCV library. It could be easily reasoned by the fact that it is better to deal with the tough things using the tool that you know and can use to reach the aim. On the contrary, {\Csharp} is a great tool with many convenient tools that help to make implementation quicker and less painful.
Multiple \ac{OpenCV} wrappers were used in the implementation on different stages of development process. \ac{EmguCV} was applied later because open source wrapper OpenCvSharp that was good enough on the whole way before was not able to do specific operations for the \ac{PTAM} code base thus it was replaced.

All various logic units were split into the separate static classes for exact purposes. The project was also split into some set of libraries projects in order to separate \ac{OpenCV}, Rubik's cube and Rendering code bases. Since the project is called framework, every library can be easily substituted with any other. The project also contains 2 projects for camera calibration for different versions of \ac{EmguCV}.

\subsubsection*{Extraction}

Extraction logic is contained in $RubiksCube.OpenCV$ project and its $FaceDetector$ and $FaceExtractor$ classes. The workflow is as follows $FaceDetector$ calls another utility class $ContourUtil$ to find specific contours. These specific contours are those who passes specified in the configuration threshold values and whose approximation shows 4 corners and convex structure. Afterwards, found contours are distinguished by the center of mass, it means that contours with similar or close enough center of mass are rejected to keep only one of them. This is done with \ac{OpenCV} method $Cv2.Moments$ and some magical {\Csharp} \ac{Linq} code. After that, filtered contours are checked for having the same angle to the horizon. With 9 distinct contours under the same angle it is supposed that 9 face pieces are found. They form a cube face thus it is needed to find 4 extremums of the group which are face corners. If some points appear to be the same or to close then additional filtering logic is applied. It is needed to extract the found face thus $FaceExtractor$ utility class is called to extract \ac{OpenCV} $Mat$ class containing extracted and transformed in perspective face image. With the help of $FaceUniquenessDetector$ utility class extracted $Mat$ object is checked for its originality. It is done with \ac{OpenCV} $Cv2.CalcHist$ and $Cv2.CompareHist$ methods, in short. If the correlation between the current face and any of the previous ones is too high, the current face is not unique. The last step is extracting colors from unique faces and forming some data structure to pass it to the next stage. $ColorsExtractor$ utility class is used to extract colors. It has to segment a face into specified, in the configuration, number of pieces and take color probe from the center pixel of each piece and convert it to the .NET $Color$ class. Later this colors are compared to six possible cube face colors and converted to the closest to them using $ClosestColorHue$ of $ColorsExtractor$. That is it, extracted colors are sent to be mapped to the Rubik's cube operational and rendering models.
\ac{PTAM} was another implemented algorithm but since its code was initially written with C++ and some currently unsupported libraries it was decided to write another simplified version of \ac{PTAM}. It does not allow to track key points when they are out of the point of view but while camera sees them everything is as designed, all main stages are included. \ac{PTAM} like approach is implemented in $RubiksCube.OpenCV.TestCase$ $PtamLileAlgorithm$ class split into 3 main stages: bootstrapping, bootstrap tracking and tracking. The most interesting for extraction stage is bootstrap tracking logic. Firstly, it computes optical flow with $CvInvoke.CalcOpticalFlowPyrLK$, then it validates if enough key points survived optical flow. Homography validation follows next to check how many key points at the current step survived in comparison with the initial value, if this value fails threshold, the process is considered to be failed. Secondly, stereo vision algorithms used. It estimates the rigid transformation with $CvInvoke.EstimateRigidTransform$ to validate camera motion between bootstrapped image and the current one. If the motion is sufficient it performs points triangulation. It is very interesting process, implemented in $OpenCvUtilities$ class. It finds fundamental matrix with $CvInvoke.FindFundamentalMat$ and checks how many key points survived. After this, essential matrix is computed from the intrinsic camera parameters and fundamental matrix. Essential matrix is decomposed with \ac{SVD} decomposition and produces two rotation matrices and two translation vectors. These results are mixed into four possible projection matrices. To check what projection matrix is the right one triangulation procedure is run until the matrix is found. This process is also very important, at this point 3D points are projected back to the image and re-projection error is found. It helps to understand how many points survived and successfully were triangulated thus can be used for tracking. Finding 3D points is important because the aim is to find information about face plane in 3D space. With the help of \ac{PCA} using $CvInvoke.PCACompute$ it finds eigenvectors and mean of the known 3D point matrix. Afterwards, \ac{PCA} of $Accord$ library is used because $EmguCV$ does not return eigenvalues. Retrieved data is used to find planes normal and its inliers, 3D points that lie in the same plane. Described analyses is what was needed for extraction logic.

\subsubsection*{Tracking}

\ac{PTAM} like approach makes tracking possible. In comparison with bootstrap tracking of a plane simple tracking is done much easier. It also uses optical flow with the same code base as in the previous step. The difference is that it does not need to triangulate points again, it uses already found 3D point and their 2D position on the current image.

\subsubsection*{Mapping}

Extracted colors should be mapped to the cube model in order to solve and render it. Data is mapped to $ScrarchEngine.Libraries.RubiksCube.Models.RubiksCubeModel$ class that represents a Rubik's cube in the project. This model has multiple methods for rotating cube's layers, flipping it or checking for cubes consistency. The mapping also includes face orientation analyses implemented through the iterative face orientation checkup. Faces positions are simply determined on the basis of their central pixel relation.

\subsubsection*{Solving}

Beginner solving algorithm was implemented to solve any 3 by 3 Rubik's cube. $BeginnerSolver$ class is inherited from $BaseSolver$ thus new solver can be easily created and applied. To simplify and replace solution hard-coding declarative \ac{JSON} structure was created. Solution formulas now could be written in \ac{JSON} where the name of the formula, its phase, state that should be to run the formula and moves written in specific globally known notation.

\begin{figure} [!ht]
  \centering    
    \lstset{style=sharpc}
        \begin{lstlisting}
            [
               {
                    "Name": "Up Front cubie on right|front",
                    "Phase": "FirstCross",
                    "IsFinal": true,
                    "StateFrom": {
                      "Right": [
                        [ null, null, null ],
                        [ "Up", null, null ],
                        [ null, null, null ]
                      ],
                      "Front": [
                        [ null, null, null ],
                        [ null, null, "Front" ],
                        [ null, null, null ]
                      ]
                    },
                    "Moves": "F"
                  },
            ]     
        \end{lstlisting}
  \caption{Example of json formula declaration.}
  \label{jsonsolformuladecl}
\end{figure}

As can be seen from the listing \ref{jsonsolformuladecl} state is declared as faces where at some positions should be specified face names whose color is on this face. For instance, from the above listing, Up-Front cubie is on the Right-Front sides thus it is written like this and if the cube front layer will be rotated clockwise the cubie will be placed where it should be. $IsFinal$ flag saying that this algorithm puts cubie in its place and no more formulas for it should be used. In other case, the solver will run formulas until the cubie is in place.
On the other hand, this \ac{JSON} notation is not yet perfect, since algorithms are for humans, cube must be flipped and rotated to run formulas when handling cube at some specific position. Currently, it is necessary to write some code within the solver class inherited from $BaseSolver$ for specific algorithm stages or flips, they cannot be reflected in \ac{JSON} yet.

\subsubsection*{Rendering}

To render the cube self-written software rendering engine oriented on cube was used. $RenderingControl$ was created with inheritance from $UserControl$. This class handles rendering and mouse handling split into partial classes. This class encapsulates rendering and updating loops with abstract $Update(Action<IEnumerable<T>> set)$ and $Render(Graphics g, IEnumerable<T> frame)$ methods used to write, basically, the logic of the game. Rendering control allows to zoom and rotate displayed image. To render the cube $RubiksCubeModel$ is split into cubies, imitating real puzzle pieces and each of them is converted to the 3D cube while only visible pieces are colored, others are black. Every cube is rotated with respect to its position, it means layers containing it. These manipulations are performed in $GenerateCubes3D$ and the result is a list of 3D cubes with specific position and rotation in 3D. Afterwards, cubes are projected on the screen and polygons are returned. At each step of update loop described transformation code is performed to translate $RubiksCubeModel$ to the list of polygons represented as $Face3D$ in order to be rendered with render loop.

\end{document}