\documentclass[../../main]{subfiles}
\graphicspath{{images/rubiks-exp/}}

\begin{document}

At this point, there was an idea to use planes as an alternative to contours approach. \ac{PTAM} was chosen for finding planes and in future it should become the mechanism for an object tracking due to its nature.

\subsubsection{Bootstrapping}

This step is the first one and the most simple. It just stores initial data, for instance, extracted key points of the first image, grayscale first image and sets the pipeline to move on. All the magic happens on next step.

\subsubsection{Bootstrap tracking}

This part of the algorithm is considered to be the most important. It should analyze images from a video stream or an image sequence by comparing it to the first image from the bootstrap step. As a result 3D point cloud is created, basically its simplified version. In addition, \ac{PTAM} is the algorithm for augmented reality thus it is possible to project different objects onto the detected surface.

\paragraph*{Optical flow and homography filtering.}

In order to know where the key points of the previous image will appear on the next image this algorithm uses "Optical flow" algorithm. After that it checks if 80 percents of the points have survived, if not it stops bootstrapping and says that tracking failed. If this condition is satisfied and more than 4 points are tracked then algorithm applies homography, it takes bootstrap key points and tracked key points. Using the mask it checks how many points survived homography and filters the lists of bootstrapped and tracked key points again so that these lists contain only the most precise key points. Then it analyzes camera motion using key points from the bootstrap step and currently tracked ones, it checks sufficient motion with \ac{OpenCV}'s function $estimateRigidTransform$. If all the conditions are satisfied it moves to the next step.

\paragraph*{Extracting essential matrix from fundamental.}

During this step it finds fundamental matrix using the bootstrapped and tracked key points. In computer vision, the fundamental matrix is a 3 by 3 matrix that relates corresponding points in stereo images. Then it filters key points again. The next step is computing essential matrix. In computer vision, the essential matrix is a 3 by 3 matrix, with some additional properties, which relates corresponding points in stereo images assuming that the cameras satisfy the pinhole camera model. The algorithm needs to have intrinsic parameters of the camera before it starts the work because it does not include calibration part. Intrinsic parameters are physical camera parameters like focal length and etc. To compute essential matrix it multiplies transposed intrinsic matrix by fundamental matrix and multiplies this by intrinsic matrix, not transposed. This matrix is needed to extract camera rotation and translation in relative perspective from the bootstrapped image to the current one. In order to do this it decomposes essential matrix with \ac{SVD}. Decomposition results into 2 possible rotation matrices and 2 possible translation vectors. If the determinant of the first rotation matrix plus $1.0f$ is less than $1e-09$ than algorithm changes signs of all numbers of the essential matrix and computes decomposition again due to “Showing that it is valid” chapter of essential matrix wiki page. Then it proceeds to the next step.

\paragraph*{Triangulation of points.}

During this step it assumes that the first image (camera) extrinsic parameters matrix and the second one are constructed from the rotation matrices and translation vectors. The resulting matrix is 3 by 4. Since there are 2 rotation matrices and 2 translation vectors, the algorithm creates 4 possible extrinsic matrices for the second camera and tries to triangulate until the correct one is found or fails if none was found. In order to triangulate, it normalizes bootstrapped and tracked key points coordinates. Then it proceeds triangulation and computes status by checking $z$ component of the points. It filters key points by this status array. After that it computes reprojection error. If the error is not in the acceptable range it runs triangulation again with different set of rotation and translation components. When the error is acceptable it goes to the next step where it finds the plane.

\paragraph*{Finding a plane using 3d point cloud.}

This step contains \ac{PCA}. It is used to extract normal of plane and proceed key points filtering again. If more than 75 percents of points are on the same plane bootstrap tracking is considered to be finished.

\subsubsection{Tracking}

This step is the final for this algorithm. It calculates optical flow for the points that passed triangulation. Then it solves \ac{Pnp} in order to calculate Model-View matrix for the rendering. That is it, simple augmented reality.
The main difference of this algorithm in comparison with the real \ac{PTAM} is that \ac{PTAM} is tracking all captured plane points even if they disappear from the camera view when you put camera back it still sees those points. On the contrary, described approach simply cuts down all unseen key points until the minimal amount exists, when it is not satisfied it breaks. This simplification significantly reduces time for implementation and understanding of the algorithm work flow.

\subsubsection{Planes for a Rubik's cube}

The idea behind the \ac{PTAM} usage was to find all contiguous planes where every 3 contiguous planes with 90 degrees angle between each other create a Rubik's cube angle. For this purpose, this triplets are used to find planes intersection (cube's corners). Using the triangulated data it is possible to find 3D coordinate for any 2D point in the specific view. These manipulations are useful in two senses, they allow to find a Rubik's cube corners and to track those corners in the 3D space, after some algorithm modification.

\begin{figure} [ht]
    \begin{center}
        \includegraphics[width=200pt]{planes_intersection}
        \caption{Example of planes intersection, cube's corner.}
        \label{fig:planes_intersection}
    \end{center}
\end{figure}

\subsubsection{Conclusion}

Algorithm performed very well for finding and tracking a single Rubik's cube face but it showed pretty bad results for the continuous running of the algorithm to find contiguous planes. The reason for that is illumination. The algorithm was able to find one, two or three planes in a row but it definitely will brake somewhere in the middle. This causes problems for triangulation stage, since the algorithm world space is relative to the first bootstrap thus if the algorithm fails the next bootstrap will become new first bootstrap, in simple worlds, algorithm is lost and it has to start from scratch. It is possible to modify this algorithm to handle such type of problems but that is the topic for one more research.

\end{document}