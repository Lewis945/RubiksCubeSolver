\documentclass[../../main]{subfiles}
\graphicspath{{images/related/semester3/}}

\begin{document}

Since there wasn’t found any projects that have the final goal as mine, in this chapter PTAM like approach is covered as a part of final project.

\subsubsection{Bootstrapping}
This step is the first one and the most simple. It just stores initial data, for instance, extracted key points of the first image, grayscale first image and sets the pipeline to move on. All the magic happens on next step.

\subsubsection{Bootstrap tracking}
This part of algorithm is considered to be the most important. It should analyze images from video stream by comparing it to the first image from bootstrap step. As a result we will get 3d point cloud, basically it’s simplified version. Since we now know the 3d structure of the object we can project objects on our detected plane. Since this algorithm is made for augmented reality solutions.
This step consists of 4 major sub steps:

\paragraph{Optical flow and homography filtering.}
In order to know where key points of a previous image will appear on a next image algorithm uses Optical flow algorithm. After that it checks if 80 percent of points have survived, if not it stops bootstrapping and says that tracking failed. If this condition is satisfied and we are tracking more than 4 points algorithm applies homography, it takes bootstrap key points and tracking ones. The using mask it checks how many points survived homography and filters the lists of bootstrapped and tracking key points again so that these lists contain only the most precise key points. Then it analyzes camera motion using key points form bootstrap step and currently tracking ones, it checks sufficient motion with OpenCV’s function estimateRigidTransform. If all the conditions are satisfied it moves to the next step.

\paragraph{Extracting essential matrix from fundamental.}
During this step it finds fundamental matrix using bootstrapped and tracked key points. In computer vision, the fundamental matrix is a 3 by 3 matrix that relates corresponding points in stereo images [3]. Then it filters key points again. The next step is computing Essential matrix. In computer vision, the essential matrix is a 3 by 3 matrix, with some additional properties, which relates corresponding points in stereo images assuming that the cameras satisfy the pinhole camera model [4]. The algorithm needs to get intrinsic parameters of the camera on its setup since it does not include calibration part. Intrinsic parameters is physical camera parameters like focal length and so on. To compute essential matrix it multiplies transposed intrinsic matrix by fundamental matrix and multiplies this by intrinsic matrix, not transposed. This matrix is needed to extract camera rotation and translation in relative perspective from bootstrapped image to the current one. In order to do this it decomposes essential matrix with SVD (Singular value decomposition) [5]. Decomposition results into 2 possible rotation matrices and 2 possible translation vectors. If the determinant of first rotation matrix plus 1.0f is less than 1e-09 than algorithm changes signs of all numbers of essential matrix and computes decomposes it again due to “Showing that it is valid” chapter of essential matrix wiki page [4]. Then it proceeds to the next step.

\paragraph{Triangulation of points.}
During this step it assumes that the first image (camera) extrinsic parameters matrix and the second one is constructed from rotation matrices and translation vectors. The resulting matrix is 3 by 4. Since there are 2 rotation matrices and 2 translation vectors algorithm creates 4 possible extrinsic matrices for the second camera and tries to triangulate until the correct one is found or fails if none found. In order to triangulate it normalizes bootstrapped and tracking key points coordinates. Then it proceeds triangulation and computes status by checking z component of points. It filters key points by this status array. After that it computes reprojection error. If the error is not in the acceptable range it runs triangulation again with different set of rotation and translation components. When error is acceptable it goes to the next step where it finds plane.

\paragraph{Finding plane using 3d point cloud.}
This step contains PCA (Principal component analysis) [6]. It is used to extract normal of plane and proceed key points filtering again. If more than 75 percent of points are on the same plane bootstrap tracking is considered to be finished.

\subsubsection{Tracking}
This step is the final for this algorithm. It simply calculated optical flow from the point when triangulation succeeded to the current image. Then it solves PnP (Perspective-n-Point) [6] in order to calculate Model-View matrix for OpenGL rendering. That is it, simple augmented reality.
The main difference of this algorithm from PTAM [?] is that PTAM is tracking all captured plane points even if they disappear from camera view when you put camera back it still sees that points. On the contrary, described approach simply cuts down all unseen key points until the minimal amount exists, when it is not satisfied it breaks.

\end{document}